{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transformer.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNakXs63UVl/9j0z4wjNJKl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"8CPFWSScW-ds"},"source":["import torch\n","import torch.nn as nn\n","\n","class Transformer(nn.module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    self.encoder = encoder()\n","    self.decoder = decoder()\n","\n","  def forward(self,encoder_input, decoder_input):\n","    \n","    '''\n","    마스크를 미리 생성한 뒤,\n","    넣어줄지, 각 encoder, decoder에서 생성할지 고민\n","    '''\n","    en_output = self.encoder(encoder_input)\n","    output = self.decoder(decoder_input, en_output)\n","\n","    return output\n","\n","  ## 보류\n","  def get_att_mask(size):\n","    return mask\n","    "],"execution_count":null,"outputs":[]}]}