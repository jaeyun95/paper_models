{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"decoder.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPEdQEM9ayMCk1epiGvtvNn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"Q_IV75b8Lz8i"},"source":["import torch\n","import torch.nn as nn\n","\n","class Decoder(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    self.decoder_embedding = nn.Embedding()\n","    pos_table = torch.FloatTensor(get_sinusoid_encoding_table(word_sequence, hight))\n","    self.position_embedding = nn.Embedding.from_pretrained(pos_table, freeze=True)\n","    self.att_layers = nn.ModuleList([EncoderLayer() for _ in range(6)])\n","\n","  def forward(self,encoder_input, decoder_input, dec_att_mask, dec_enc_att_mask):\n","\n","    en_output = self.encoder(encoder_input)\n","    outputs = self.decoder(decoder_input, en_output)\n","\n","    dec_att_probs, dec_enc_att_probs = []\n","    for layer in self.att_layers:\n","      outputs = layer(decoder_input, emd_inputs, dec_att_mask, dec_enc_att_mask)\n","      dec_att_probs.append(outputs['dec_att_prob'])\n","      dec_enc_att_probs.append(outputs['dec_enc_prob'])\n","\n","    return {'outputs': outputs['outputs'], 'dec_att_prob': dec_att_prob, 'dec_enc_prob': dec_enc_prob}\n","\n","    "],"execution_count":null,"outputs":[]}]}