# -*- coding: utf-8 -*-
"""encoder_layer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15dB2PIryimu0YViTd5PP9oUBnE9JQZsf
"""

import torch
import torch.nn as nn
import math 

class EncoderLayer(nn.Module):
  def __init__(self):
      super().__init__()

      self.self_attention = MultiHeadAttention()
      self.layer_norm = nn.LayerNorm()
      self.position_wise = PositionWiseFeedForward()
      self.layer_norm2 = nn.LayerNorm()

  def forward(self, inputs, att_mask):
    
    outputs, att_prob = self.self_attention(inputs, inputs, inputs, att_mask)
    outputs = self.layer_norm(inputs + att_output)
    outputs = self.position_wise(outputs)
    outputs = self.layer_norm(inputs + outputs)
    
    return {'output': outputs, 'att_prob': att_prob}

!git clone https://github.com/jaeyun95/paper_models.git