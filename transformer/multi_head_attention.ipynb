{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"multi_head_attention.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOltfAFp5VEIEpbxxtAI3az"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jAXCu2piL0wK","executionInfo":{"status":"ok","timestamp":1616423482852,"user_tz":-540,"elapsed":608,"user":{"displayName":"이재윤","photoUrl":"","userId":"10578467085122722810"}},"outputId":"a167c495-18c3-4159-c4f5-beded4193f7c"},"source":["import torch\n","import torch.nn as nn\n","import math \n","\n","class MultiHeadAttention(nn.Module):\n","  def __init__(self):\n","      super().__init__()\n","      \n","      self.q_weight = nn.Linear()\n","      self.k_weight = nn.Linear()\n","      self.v_weight = nn.Linear()\n","      self.scaled_dot_product_att = ScaledDotProductAttention()\n","      self.fc = nn.Linear()\n","      \n","  def forward(self, query, key, value, att_mask):\n","    \n","    # 각각을 head에 맞게 쪼개주는 과정\n","    query = query //\n","    key = key //\n","    value = value //\n","    att_mask = att_mask //\n","\n","    outputs = self.scaled_dot_product_att(query, key, value, att_mask)\n","    final_outputs = self.fc(outputs['context'])\n","\n","     return {'outputs':final_outputs, 'att_prob':outputs['att_prob']}\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[ 2.7491,  0.5190,  0.6002,  1.3882,  1.7914],\n","        [-0.7195,  0.6810, -0.5994,  0.0169,  0.2861],\n","        [ 0.2133, -0.2985, -0.2023,  0.1049, -1.1491]])\n","tensor([[ 2.7413,  0.5182,  0.5976,  1.3847,  1.7857],\n","        [ 0.0767,  0.4157, -0.3059,  0.2633,  0.1832],\n","        [ 0.3723,  0.0611, -0.1765,  0.2779, -0.3585]])\n"],"name":"stdout"}]}]}