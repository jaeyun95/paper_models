{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"decoder_layer.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPHmXNJz7rN11y+ULbfreuK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"J2ZO5L-XTtXu"},"source":["import torch\n","import torch.nn as nn\n","import math \n","\n","class DecoderLayer(nn.Module):\n","  def __init__(self):\n","      super().__init__()\n","\n","      self.decoder_embedding = MultiHeadAttention()\n","      self.self_attention = MultiHeadAttention()\n","      self.layer_norm = nn.LayerNorm()\n","      self.position_wise = PositionWiseFeedForward()\n","      self.layer_norm2 = nn.LayerNorm()\n","      self.layer_norm3 = nn.LayerNorm()\n","\n","  def forward(self, inputs, encoder_outputs, dec_att_mask, dec_enc_att_mask):\n","    \n","    decoder_output, dec_att_prob = self.decoder_embedding(inputs, inputs, inputs, att_mask)\n","    decoder_output = self.layer_norm(inputs + decoder_output)\n","\n","    dec_enc_output, dec_enc_prob = self.self_attention(decoder_output, encoder_outputs, encoder_outputs)\n","    dec_enc_output = self.layer_norm2(decoder_output + dec_enc_output)\n","\n","    outputs = self.position_wise(dec_enc_output)\n","    outputs = self.layer_norm3(dec_enc_output + output)\n","\n","    return {'output': outputs, 'dec_att_prob': dec_att_prob, 'dec_enc_prob': dec_enc_prob}\n","\n"],"execution_count":null,"outputs":[]}]}