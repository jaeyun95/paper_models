# -*- coding: utf-8 -*-
"""multi_head_attention.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sd_pFdQnQfXOu227exHmJ98P2AYHQ3z-
"""

import torch
import torch.nn as nn
import math 

class MultiHeadAttention(nn.Module):
  def __init__(self):
      super().__init__()
      
      self.q_weight = nn.Linear()
      self.k_weight = nn.Linear()
      self.v_weight = nn.Linear()
      self.scaled_dot_product_att = ScaledDotProductAttention()
      self.fc = nn.Linear()
      
  def forward(self, query, key, value, att_mask):
    
    # 각각을 head에 맞게 쪼개주는 과정
    query = query //
    key = key //
    value = value //
    att_mask = att_mask //

    outputs = self.scaled_dot_product_att(query, key, value, att_mask)
    final_outputs = self.fc(outputs['context'])

     return {'outputs':final_outputs, 'att_prob':outputs['att_prob']}