# -*- coding: utf-8 -*-
"""scaled_dot_product.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I06OImpyc-dJBQAhPs59boeddIlRaI-C
"""

import torch
import torch.nn as nn
import math 

class ScaledDotProductAttention(nn.Module):
  def __init__(self):
      super().__init__()
      
  def forward(self, query, key, value, att_mask):
    d = query.size(-1)
    att_score = torch.matmul(query, key.transpose(-2,-1))
    att_score = att_score/math.sqrt(d)
    if att_mask is not None:
      att_score.masked_fill_(att_mask, -1e9)
    att_prob = nn.Softmax(dim=-1)(att_score)
    context_info = torch.matmul(att_prob, value)
    return {'context':context_info, 'att_prob':att_prob}