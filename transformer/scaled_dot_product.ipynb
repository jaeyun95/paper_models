{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"scaled_dot_product.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNs5OtDcZ9qp2+5udSBP1Ws"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"n1r1PUQyUBxJ"},"source":["import torch\n","import torch.nn as nn\n","import math \n","\n","class ScaledDotProductAttention(nn.Module):\n","  def __init__(self):\n","      super().__init__()\n","      \n","  def forward(self, query, key, value, att_mask):\n","    d = query.size(-1)\n","    att_score = torch.matmul(query, key.transpose(-2,-1))\n","    att_score = att_score/math.sqrt(d)\n","    if att_mask is not None:\n","      att_score.masked_fill_(att_mask, -1e9)\n","    att_prob = nn.Softmax(dim=-1)(att_score)\n","    context_info = torch.matmul(att_prob, value)\n","    return {'context':context_info, 'att_prob':att_prob}\n","\n"],"execution_count":null,"outputs":[]}]}