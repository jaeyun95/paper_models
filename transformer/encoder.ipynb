{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"encoder.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPfUmEHqkdA45DYXL1dDxKa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"sVHhV5VKLymy"},"source":["import torch\n","import torch.nn as nn\n","\n","class Encoder(nn.module):\n","  def __init__(self):\n","    super().__init__()\n","\n","    self.emgedding = nn.Embedding()\n","    pos_table = torch.FloatTensor(get_sinusoid_encoding_table(word_sequence, hight))\n","    self.position_embedding = nn.Embedding.from_pretrained(pos_table, freeze=True)\n","    self.att_layers = nn.ModuleList([EncoderLayer() for _ in range(6)])\n","\n","  def forward(self,inputs, att_mask):\n","    \n","    emd_inputs = self.emnedding(inputs) + self.pos_embedding(positions)\n","    att_probs = []\n","    for layer in self.att_layers:\n","      outputs = layer(emd_inputs, att_mask)\n","      att_probs.append(outputs['att_probs'])\n","\n","    return {'outputs': outputs['output'], 'att_probs': att_probs}\n","\n","\n","    "],"execution_count":null,"outputs":[]}]}