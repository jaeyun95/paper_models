# Paper Models


1. [Attention Is All You Need](https://blog.naver.com/jaeyoon_95/222280537045) [code](https://github.com/jaeyun95/paper_models/tree/master/transformer)   
2. [Improving Language Understanding by Generative Pre-Training](https://blog.naver.com/jaeyoon_95/222307328375) 
3. [BERT: Pre-training of Deep Bidirectional Transformers forLanguage Understanding](https://blog.naver.com/jaeyoon_95/222307615069) 
4. [Deep contextualized word representations](https://blog.naver.com/jaeyoon_95/222308499404) 
5. [Language Models are Unsupervised Multitask Learners](https://blog.naver.com/jaeyoon_95/222308668625) 


